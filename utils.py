import numpy as np
import yolo
import cv2
import pyrealsense2 as rs
import torch


def pixel_to_3d(u, v, depth_val, fx, fy, cx, cy):
    X = (u - cx) * depth_val / fx
    Y = (v - cy) * depth_val / fy
    Z = depth_val
    return X, Y, Z
    
# def stairs(bbox):
#     angle =
#     height = 
#     distance = 

#----------------- ë°ì´í„° ë¡œë“œ í•¨ìˆ˜

def align_depth_to_rgb(depth_bin_path, rgb_bin_path, frame_idx, height=480, width=640):
    """
    Realsense ì¥ë¹„ê°€ ì—°ê²°ë˜ì–´ ìˆìœ¼ë©´ ì •ë ¬ëœ Depth & RGB í”„ë ˆì„ì„ ê°€ì ¸ì˜¤ê³ ,
    ì—°ê²°ë˜ì§€ ì•Šì•˜ì„ ê²½ìš° .bin íŒŒì¼ì—ì„œ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.

    Parameters
    ----------
    depth_bin_path : str
        - Depth .bin íŒŒì¼ ê²½ë¡œ
    rgb_bin_path : str
        - RGB .bin íŒŒì¼ ê²½ë¡œ
    frame_idx : int
        - ê°€ì ¸ì˜¬ í”„ë ˆì„ ì¸ë±ìŠ¤
    height : int, default=480
        - í”„ë ˆì„ ë†’ì´ (Realsense ê¸°ë³¸ê°’)
    width : int, default=640
        - í”„ë ˆì„ ë„ˆë¹„ (Realsense ê¸°ë³¸ê°’)

    Returns
    -------
    depth_map : np.ndarray (H, W) (float32)
        - RGB ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ëœ Depth Map (ë˜ëŠ” .binì—ì„œ ë¶ˆëŸ¬ì˜¨ Depth Map)
    rgb_image : np.ndarray (H, W, 3) (uint8)
        - RGB ì´ë¯¸ì§€ (BGR)
    """
    context = rs.context()
    devices = context.query_devices()

    if len(devices) == 0:
        print("ğŸ”¹ No device connected, using default intrinsics & loading from .bin files")
        intrinsics = rs.intrinsics()
        intrinsics.width = width
        intrinsics.height = height
        intrinsics.ppx = 308.5001  # ê¸°ë³¸ ê´‘í•™ ì¤‘ì‹¬ X (cx)
        intrinsics.ppy = 246.4238  # ê¸°ë³¸ ê´‘í•™ ì¤‘ì‹¬ Y (cy)
        intrinsics.fx = 605.9815  # ê¸°ë³¸ ì´ˆì  ê±°ë¦¬ X (fx)
        intrinsics.fy = 606.1337  # ê¸°ë³¸ ì´ˆì  ê±°ë¦¬ Y (fy)
        intrinsics.model = rs.distortion.none  # ì™œê³¡ ì—†ìŒ
        intrinsics.coeffs = [0, 0, 0, 0, 0]  
        
        # --- .bin íŒŒì¼ì—ì„œ RGB & Depth ë¶ˆëŸ¬ì˜¤ê¸° ---
        depth_data = np.fromfile(depth_bin_path, dtype=np.float32)
        rgb_data = np.fromfile(rgb_bin_path, dtype=np.uint8)

        total_frames = len(depth_data) // (height * width)
        if frame_idx >= total_frames:
            raise ValueError(f"âš ï¸ frame_idx {frame_idx}ê°€ ì €ì¥ëœ í”„ë ˆì„ ê°œìˆ˜ {total_frames}ë³´ë‹¤ í¼")

        # íŠ¹ì • í”„ë ˆì„ ì¶”ì¶œ
        start_idx = frame_idx * height * width
        depth_map = depth_data[start_idx : start_idx + (height * width)].reshape((height, width))
        
        start_idx = frame_idx * height * width * 3
        rgb_image = rgb_data[start_idx : start_idx + (height * width * 3)].reshape((height, width, 3))

    else:
        try:
            print("âœ… Realsense device detected, capturing frames...")
            pipeline = rs.pipeline()
            config = rs.config()
            config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
            config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
            profile = pipeline.start(config)

            # ì¹´ë©”ë¼ Intrinsics ê°€ì ¸ì˜¤ê¸°
            color_profile = profile.get_stream(rs.stream.color)
            intr = color_profile.as_video_stream_profile().get_intrinsics()
            fx, fy, cx, cy = intr.fx, intr.fy, intr.ppx, intr.ppy

            # Depth â†’ RGB ì •ë ¬ ìˆ˜í–‰
            align_to = rs.stream.color
            align = rs.align(align_to)

            # í”„ë ˆì„ ìˆ˜ì§‘ ë° ì •ë ¬
            frames = pipeline.wait_for_frames()
            aligned_frames = align.process(frames)

            depth_frame = aligned_frames.get_depth_frame()
            color_frame = aligned_frames.get_color_frame()

            if not depth_frame or not color_frame:
                raise RuntimeError("âš ï¸ Failed to capture frames from Realsense.")

            # numpy ë°°ì—´ ë³€í™˜
            depth_map = np.asanyarray(depth_frame.get_data()).astype(np.float32)
            rgb_image = np.asanyarray(color_frame.get_data())

            pipeline.stop()

        except RuntimeError:
            print("âŒ No device connected (error during capture), using default intrinsics")
            profile = None
            depth_map = np.zeros((height, width), dtype=np.float32)  # ë¹ˆ Depth ë§µ ìƒì„±
            rgb_image = np.zeros((height, width, 3), dtype=np.uint8)  # ë¹ˆ RGB ì´ë¯¸ì§€ ìƒì„±

    return depth_map, rgb_image











#-----------
def segment_stairs_in_roi(color_img, bbox, model, device='cuda'):
    """
    ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬, bbox ë‚´ ê³„ë‹¨ ì˜ì—­ì„ ë§ˆìŠ¤í‚¹í•˜ëŠ” í•¨ìˆ˜.
    ë‹¨, bboxë¥¼ ì„¸ë¡œë¡œ 3ë“±ë¶„í•œ ë’¤, í•˜ë‹¨ 2/3ë§Œ ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰í•˜ì—¬ ì—°ì‚°ëŸ‰ì„ ì¤„ì„.

    Params
    ------
    color_img: np.ndarray
        - ì›ë³¸ ì»¬ëŸ¬ ì´ë¯¸ì§€ (H, W, 3) (BGR ë˜ëŠ” RGB)
    bbox: (x1, y1, x2, y2)
        - YOLOë¡œë¶€í„° ë°›ì€ ê³„ë‹¨ ROI ì¢Œí‘œ
    model: torch.nn.Module (ì˜ˆì‹œ)
        - ì„¸ê·¸ë©˜í…Œì´ì…˜ PyTorch ëª¨ë¸(ì˜ˆ: U-Net, DeepLab, etc.)
    device: str
        - 'cuda' ë˜ëŠ” 'cpu'
    
    Returns
    -------
    mask_full: np.ndarray (H_roi, W_roi)
        - ROI ì „ì²´ í¬ê¸°ì— ëŒ€ì‘í•˜ëŠ” ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ (0=ë°°ê²½, 1=ê³„ë‹¨ ...)
        - ìƒë‹¨ 1/3 êµ¬ê°„ì€ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ 0(ë°°ê²½) ì²˜ë¦¬ë¨
    """

    x1, y1, x2, y2 = bbox
    roi_color = color_img[y1:y2, x1:x2]  # ROI ì¶”ì¶œ

    # --- 1) ROIë¥¼ ì„¸ë¡œë¡œ 3ë“±ë¶„ ---
    roi_h, roi_w = roi_color.shape[:2]
    one_third = roi_h // 3  # 3ë“±ë¶„ ë†’ì´
    # í•˜ë‹¨ 2/3 êµ¬ê°„: (one_third, roi_h)
    sub_roi_color = roi_color[one_third: , :]

    # --- 2) ëª¨ë¸ ì…ë ¥ ì „ì²˜ë¦¬(ì˜ˆì‹œ) ---
    # ëª¨ë¸ì— ë”°ë¼ Resize, Normalize ë“±ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ
    # ì˜ˆ: (H, W, 3) BGR -> (1, 3, H, W) RGB í…ì„œ
    sub_roi_rgb = cv2.cvtColor(sub_roi_color, cv2.COLOR_BGR2RGB)
    sub_roi_tensor = torch.from_numpy(sub_roi_rgb).float().permute(2,0,1).unsqueeze(0)  # (1,3,h,w)
    # ê°„ë‹¨íˆ 0~1 ìŠ¤ì¼€ì¼ë§
    sub_roi_tensor = sub_roi_tensor / 255.0
    sub_roi_tensor = sub_roi_tensor.to(device)

    # --- 3) ì„¸ê·¸ë©˜í…Œì´ì…˜ ìˆ˜í–‰ ---
    model.eval()
    with torch.no_grad():
        pred = model(sub_roi_tensor)  # (1, num_classes, h, w) í˜•íƒœ ê°€ì •
        # ì˜ˆ: ì±„ë„ ì°¨ì›ì—ì„œ argmax -> í´ë˜ìŠ¤ ë§µ
        pred_mask = torch.argmax(pred, dim=1).squeeze(0)  # (h, w)
    
    sub_roi_mask = pred_mask.cpu().numpy().astype(np.uint8)  # ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ (í•˜ë‹¨ 2/3 êµ¬ê°„)

    # --- 4) ì„¸ê·¸ë©˜í…Œì´ì…˜ ê²°ê³¼ë¥¼ ROI ì „ì²´ í¬ê¸°ì— ë§ì¶° í•©ì¹˜ê¸° ---
    # ìƒë‹¨ 1/3 ë¶€ë¶„ì€ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ë°°ê²½(0)ìœ¼ë¡œ ë‘ 
    mask_full = np.zeros((roi_h, roi_w), dtype=np.uint8)
    mask_full[one_third: , :] = sub_roi_mask  # í•˜ë‹¨ 2/3 ë¶€ë¶„ë§Œ ì˜ˆì¸¡ ê²°ê³¼ ë°˜ì˜

    return mask_full


# ----------------------
# ì˜ˆì‹œ: í›„ì†ì²˜ë¦¬ (ì—ì§€ ê²€ì¶œ + ì§ì„  ê²€ì¶œ) í•¨ìˆ˜
# ----------------------

def postprocess_stair_mask(mask_full):
    """
    ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ì—ì„œ ê³„ë‹¨ ìœ¤ê³½ì„ ì¢€ ë” ê¹”ë”í•˜ê²Œ ì–»ê¸° ìœ„í•œ í›„ì† ì²˜ë¦¬ ì˜ˆì‹œ
    1) ëª¨í´ë¡œì§€ ì—°ì‚°
    2) ì—ì§€ ê²€ì¶œ(Canny)
    3) (ì„ íƒ) HoughLinesPë¡œ ê³„ë‹¨ ì—£ì§€ ì§ì„  ê²€ì¶œ
    """

    # 1) ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ ì¡ìŒ ì œê±°/ì±„ì›€
    kernel = np.ones((3,3), np.uint8)
    mask_clean = cv2.morphologyEx(mask_full, cv2.MORPH_OPEN, kernel, iterations=1)
    mask_clean = cv2.morphologyEx(mask_clean, cv2.MORPH_CLOSE, kernel, iterations=1)

    # 2) ì—ì§€ ê²€ì¶œ
    # ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ ìì²´ê°€ 0/1(or 0/255)ì´ë¯€ë¡œ, 
    # ìœ¤ê³½ì„ ì„ ì°¾ìœ¼ë ¤ë©´ Cannyë¥¼ ì ìš©í•˜ê±°ë‚˜, cv2.findContours()ë„ ê°€ëŠ¥
    edges = cv2.Canny(mask_clean, 50, 150)

    # 3) ì§ì„  ê²€ì¶œ (ì˜ˆì‹œ)
    lines_p = cv2.HoughLinesP(
        edges,
        rho=1,
        theta=np.pi/180,
        threshold=30,
        minLineLength=30,
        maxLineGap=10
    )

    # í•„ìš” ì‹œ lines_pë¥¼ í›„ì† ì²˜ë¦¬(ë³‘í•©/í•„í„°ë§) í›„, ê°ë„ ê³„ì‚° ë“±ì— í™œìš©
    return edges, lines_p


# ----------------------
# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ
# ----------------------
if __name__ == "__main__":
    # ê°€ì •: color_img (BGR)ì™€ YOLOë¡œë¶€í„° ì–»ì€ bbox, ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ ì¤€ë¹„
    color_img = cv2.imread("test.jpg")
    yolo_bbox = (100, 200, 400, 500)  # ì˜ˆì‹œ (x1, y1, x2, y2)

    # ì˜ˆ: ì„ì˜ì˜ ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸ (PyTorch)
    # model = MySegModel(...)
    # model.load_state_dict(torch.load("model.pth"))
    # device = 'cuda' if torch.cuda.is_available() else 'cpu'
    # model.to(device)

    # ì—¬ê¸°ì„œëŠ” ë°ëª¨ë¼ì„œ model ëŒ€ì‹  None ì²˜ë¦¬
    model = None
    device = 'cpu'

    # ì„¸ê·¸ë©˜í…Œì´ì…˜ í•¨ìˆ˜ ì‹œì—° (ì‹¤ì œë¡œëŠ” modelì´ í•„ìš”)
    # segment_stairs_in_roi í•¨ìˆ˜ ë‚´ model ë¶€ë¶„ì„ ì§ì ‘ ìˆ˜ì •í•´ì„œ ì‚¬ìš© ê°€ëŠ¥
    # í˜¹ì€ ì•„ë˜ì²˜ëŸ¼ "ë”ë¯¸"ë¡œ ì˜ˆì‹œë¥¼ ë§Œë“¤ ìˆ˜ë„ ìˆìŒ
    def dummy_model(x):
        # ì…ë ¥ x: (1,3,h,w)
        # ê°€ì§œë¡œ ì „ë¶€ '1' í´ë˜ìŠ¤ë¼ê³  ì¹˜ì (ì „ë¶€ ê³„ë‹¨)
        return torch.zeros((1, 2, x.shape[2], x.shape[3]), device=x.device) + 0.5

    seg_model = dummy_model

    mask_roi = segment_stairs_in_roi(color_img, yolo_bbox, seg_model, device=device)
    
    # í›„ì†ì²˜ë¦¬
    edges, lines_p = postprocess_stair_mask(mask_roi)

    # ì‹œê°í™” ì˜ˆì‹œ
    # ROI ì˜ì—­ë§Œ ì‹œê°í™”
    x1, y1, x2, y2 = yolo_bbox
    roi_vis = color_img[y1:y2, x1:x2].copy()

    # ì—ì§€ ê·¸ë¦¬ê¸°
    edges_bgr = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)
    edges_bgr[edges != 0] = (0, 0, 255)  # ë¹¨ê°„ìƒ‰

    # lines_pê°€ ìˆìœ¼ë©´ ì§ì„  ì‹œê°í™”
    if lines_p is not None:
        for line in lines_p:
            x_start, y_start, x_end, y_end = line[0]
            cv2.line(roi_vis, (x_start, y_start), (x_end, y_end), (0,255,0), 2)

    # ê²°ê³¼ ë³´ê¸°
    cv2.imshow("ROI mask", mask_roi*255)  # 0 or 1 => ì‹œê°í™”ë¥¼ ìœ„í•´ 255 ê³±
    cv2.imshow("ROI edges", edges_bgr)
    cv2.imshow("ROI lines", roi_vis)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
